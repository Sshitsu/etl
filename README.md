# ETL Pipeline (Java)

## О проекте
Здравствуйте! Меня зовут **Роман Парубов**. Я выполнял тестовое задание ETL‑pipeline для «Проект Шифт».  
Задание реализовано на **Java** **без использования фреймворков**.

## Стратегия борьбы с дубликатами: Bloom Filter
Первое, о чём хочу рассказать, — как я боролся с дубликатами.  
Для детектирования и отсечения дублей используется **Bloom Filter**.

### Почему именно Bloom Filter?
- **Быстрый доступ по ключу.** Проверка принадлежности элемента выполняется через несколько хэш‑функций и обращение к битовой матрице/хэш‑таблицам. Это даёт амортизированное время около *O(1)*. В худшем случае при большом числе коллизий возможна деградация до *O(n)*, где *n* — количество данных.
- **Выбор ключа.** В качестве ключа я использую комбинацию `latitude + longitude + date`. По моему мнению, эти три параметра однозначно идентифицируют запрос. Поэтому их же я выбрал как **PRIMARY KEY** в PostgreSQL.
- **Память и эффективность.** Bloom Filter гораздо компактнее, чем хранить все ключи в обычном множестве/таблице.
- **Компромисс.** Фильтр со временем «насыщается»: растёт вероятность ложноположительных срабатываний и/или требуется больше памяти. Это осознанная плата за скорость.
- **Сравнение.** Если каждый раз проверять наличие записи напрямую в БД или CSV при огромных объёмах, программа может работать неприемлемо долго. Bloom Filter позволяет быстро отсеивать уже виденные ключи до обращения к внешним источникам.

### Параметры Bloom Filter

Bloom Filter настраивается двумя ключевыми параметрами:

1. **Ожидаемое количество элементов (n)** — сколько уникальных записей мы планируем пропустить через фильтр.  
2. **Допустимая вероятность ложноположительного срабатывания (p)** — шанс, что фильтр «скажет», будто элемент уже был, хотя его не было.

В моём коде я задал:
```java
new BloomFilter<>(100_000, 0.001); // n = 100_000, p = 0.001 (0.1%)
```
Это рабочие значения для тестового задания.
В реальном ETL-пайплайне важно сначала оценить общий объём данных, чтобы подобрать параметры идеально (или хотя бы близко к реальности). Также стоит предусмотреть возможность динамически менять эти параметры (например, пересоздавать фильтр по расписанию, шардинговать по датам или использовать масштабируемый Bloom Filter), если поток данных растёт быстрее прогнозов.
Так же в моем коде у класса который записывает в Csv и у класса который записывает в базу данных. BloomFilter разный т.к если мы записали в csv это не значит что в базе данных есть такое значения, поэтому у них разные BloomFilter
> Ложноположительные результаты возможны, но **ложноотрицательных быть не должно** — реальный дубликат не пройдёт.

### Как это устроено в коде
1. При чтении каждой записи формируется набор хэшей от ключа (`lat+lon+date`).
2. Проверяется соответствующий набор битов в фильтре.
3. Если запись «новая» — добавляем её в фильтр и продолжаем обработку; иначе пропускаем или обновляем (в зависимости от логики).

## Что уже реализовано
- Извлечение данных по разным направлениям:
  - **API → Database**
  - **API → CSV**
  - **JSON → Database**
  - **JSON → CSV**
- **Трансформации:** приведение единиц измерения (например, Фаренгейт → Цельсий, футы → метры и т.д.).
- **Загрузка:** в базу данных **PostgreSQL** или в **CSV**. Дубликаты не допускаются (за счёт Bloom Filter и PRIMARY KEY).
- **Механизм де-дупликации:** реализован через Bloom Filter.
- **Конфигурация:** через `.properties`.

## Что осталось сделать
- Поднять Docker (Dockerfile + docker-compose для БД и приложения).
- Написать скрипт для взаимодействия с запущенным Docker-контейнером, чтобы можно было отправлять команды на запись данных.
- Перенести все «жёстко прописанные» значения (hardcode) в переменные окружения. К сожалению, не успел это завершить.
- Добавить логирование данных

## Стек
- **Java**: 17
- **Сборка:** Maven (`pom.xml`)
- **База данных:** PostgreSQL
- **Тестирование:** JUnit 5

## Структура репозитория
```
├── cache/                 # *.bloom файлы (сохранённые состояния фильтра)
├── data/                  # CSV-файлы с результатами загрузки
├── db/                    # Скрипт создания БД (init.sql)
├── src/
│   ├── main/
│   │   ├── java/          # Код ETL: extractors, transformers, loaders, Bloom Filter utils
│   │   └── resources/     # Конфигурации (.properties и др.)
│   └── test/
│       └── java/          # Тесты (JUnit 5)
└── pom.xml                # Зависимости и плагины Maven
```


## Запуск
Для начала нужно создать базу данных PostgreSql с помощью скрипта оставленного в папке etl/db/inti.sql
Далее заполнить переменные для базы данных в *application.properties*
Далее 
```
mvn clean package
java -jar target/etl-pipeline-<version>.jar
```

